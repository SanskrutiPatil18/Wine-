# -*- coding: utf-8 -*-
"""Untitled35.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bTeitEuntmWU3CTQkXT1kCpcXpUFtHrv
"""

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, adjusted_rand_score, silhouette_score

# ---------------- PAGE CONFIG ----------------
st.set_page_config(
    page_title="Wine ML Dashboard",
    page_icon="üç∑",
    layout="centered"
)

st.title("üç∑ Wine Dataset ML Dashboard")
st.caption("Logistic Regression ‚Ä¢ K-Means ‚Ä¢ PCA Visualization")

# ---------------- SIDEBAR ----------------
st.sidebar.header("‚öôÔ∏è Controls")
show_data = st.sidebar.checkbox("Show Dataset Preview")
k_value = st.sidebar.slider("Select Number of Clusters (K)", 2, 6, 3)

st.sidebar.markdown("---")
st.sidebar.info("Developed for ML Deployment Demo")

# ---------------- LOAD DATA ----------------
data = pd.read_csv("wine.csv")

X = data.drop("WineClass", axis=1)
y = data["WineClass"]

if show_data:
    st.subheader("üìÑ Dataset Preview")
    st.dataframe(data.head())

# ---------------- SCALING ----------------
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ==================================================
# LOGISTIC REGRESSION
# ==================================================
st.markdown("## üîµ 1. Logistic Regression (Supervised Learning)")

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

lr = LogisticRegression(max_iter=200)
lr.fit(X_train, y_train)

y_pred = lr.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

st.success(f"‚úÖ Classification Accuracy: **{accuracy:.2f}**")
st.caption("Supervised learning uses labeled data to predict wine classes.")

# ==================================================
# ELBOW METHOD
# ==================================================
st.markdown("## üü† 2. Elbow Method (Optimal Clusters)")

wcss = []
for k in range(1, 11):
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(X_scaled)
    wcss.append(km.inertia_)

fig1 = plt.figure()
plt.plot(range(1, 11), wcss, marker='o')
plt.xlabel("Number of Clusters")
plt.ylabel("WCSS")
plt.title("Elbow Method")
st.pyplot(fig1)

st.caption("Elbow point suggests optimal number of clusters.")

# ==================================================
# K-MEANS CLUSTERING
# ==================================================
st.markdown("## üü¢ 3. K-Means Clustering (Unsupervised Learning)")

kmeans = KMeans(n_clusters=k_value, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

ari = adjusted_rand_score(y, clusters)
sil = silhouette_score(X_scaled, clusters)

col1, col2 = st.columns(2)
col1.metric("Adjusted Rand Index", f"{ari:.2f}")
col2.metric("Silhouette Score", f"{sil:.2f}")

st.caption("Clustering finds natural groupings without using labels.")

# ==================================================
# PCA VISUALIZATION
# ==================================================
st.markdown("## üü£ 4. PCA Visualization")

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

fig2 = plt.figure()
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters)
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("Cluster Visualization using PCA")
st.pyplot(fig2)

st.caption("PCA reduces dimensions for better visualization.")

# ---------------- FOOTER ----------------
st.markdown("---")
st.markdown(
    "<center>üöÄ Deployed using Streamlit | Machine Learning Project</center>",
    unsafe_allow_html=True
)
